(window.webpackJsonp=window.webpackJsonp||[]).push([[25],{378:function(r,t,e){"use strict";e.r(t);var a=e(4),v=Object(a.a)({},(function(){var r=this,t=r.$createElement,e=r._self._c||t;return e("ContentSlotsDistributor",{attrs:{"slot-key":r.$parent.slotKey}},[e("ClientOnly",[e("title-pv")],1),r._v("\n# 文本图\n"),e("p",[r._v("在自然语言处理领域，文本图（Text Graph，或Textual Graph）是一种以图形结构表示文本信息和语义关系的数据模型。它通过将文本中的单词、短语或句子表示为图形中的节点，并使用边表示它们之间的关联关系来建模文本的结构和语义。文本图可以捕捉文本中的各种关系，例如词语之间的共现关系、句子之间的相似性、概念之间的关联、上下文关联、同义关系、上位词关系等。通过分析和建模文本中的这些关系，文本图可以提供更全面的语义信息和更丰富的上下文理解。该方法通常用于文本理解、文本分类、文本摘要、关系抽取、文本蕴含任务等。")]),r._v(" "),e("p",[r._v("文本图的节点和边的定义很宽泛。对于节点来说，其可以被视为单个单词或句子，或特定于领域的术语，或文本中提到的实体（此时退化为关系图）。对于边来说，其可以是共现关系、上下文关系、从属关系等等。")]),r._v(" "),e("p",[r._v("使用文本图有以下几个好处：")]),r._v(" "),e("ul",[e("li",[r._v("语义关联的捕捉：文本图可以捕捉文本中的语义关系，包括词语之间的关联、短语之间的相关性以及句子之间的相似性。通过建模这些语义关系，文本图可以提供更全面的语义信息，帮助模型更好地理解文本的含义。")]),r._v(" "),e("li",[r._v("上下文理解的增强：文本图可以在句子或段落的上下文中建立连接，通过边的关联性，将相关的信息联系起来。这样可以帮助模型更好地理解上下文中的信息，捕捉上下文之间的关系，从而提升模型对文本的准确理解。")]),r._v(" "),e("li",[r._v("多模态信息的整合：文本图可以与其他模态的信息（如图像、语音等）相结合，形成多模态图（Multimodal Graph）。这种整合可以在处理多模态数据时，更好地表示和建模文本与其他模态之间的关系，实现更全面的信息理解。")]),r._v(" "),e("li",[r._v("语义推理和知识抽取：通过文本图，可以进行语义推理和知识抽取。例如，通过分析文本图中的关系，可以进行推理和推断，发现隐藏的语义关系。此外，文本图还可以从文本中抽取出重要的概念和关键信息，用于知识表示和知识图谱构建。")])]),r._v(" "),e("h2",{attrs:{id:"text-gcn"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#text-gcn"}},[r._v("#")]),r._v(" Text GCN")]),r._v(" "),e("p",[r._v("论文题目：Graph Convolutional Networks for Text Classification")]),r._v(" "),e("p",[r._v("论文将图卷积神经网络GCN用于文本分类任务，构造网络模型text GCN，并在4个长文本数据集（20NG，R8,R52,Ohsumed）和1个短文本数据集(MR,电影短评论数据集)上进行了实验验证。")]),r._v(" "),e("p",[r._v("首先，论文构建了一个大型的异构文本图网络，图中包含单词节点和文档节点，以至于全局共现词可以被明确的构建出。如下图所示，文本图网络中的节点的数量为文档节点个数（数据集的大小）加上数据集中包含的不重复单词的个数（词汇表的大小）。")]),r._v(" "),e("p",[r._v("之后，论文在GCN基础上进行的改造，主要改造在于提出使用整个语料库构造成一个图，图中的节点是语料库中的每个document 和字典中所有的word，构建一个大图，并用邻接矩阵表示。构建好图及其邻接矩阵之后，使用GCN进行传递计算。")]),r._v(" "),e("p",[r._v("最后，文章使用两层GCN进行模型训练，并通过实验证明两层GCN比单层的或者两层以上GCN效果更好。最后作者在文章中对网络参数的选择进行了实验对比，也对模型的词向量学习进行了可视化表现，在多方面证明了GCN在文档分类上的有效性。这说明GCN中的图及其邻接矩阵可以用多种方式进行构建，比如词之间的相关程度，词与文档的相关程度，甚至是依赖树等任何你能想到的方式，所以提出新的图构建方式也是GCN在文档分类上的一种创新。")]),r._v(" "),e("h2",{attrs:{id:"graph-transformer"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#graph-transformer"}},[r._v("#")]),r._v(" Graph Transformer")]),r._v(" "),e("p",[r._v("论文题目：Graphormers: Do Transformers Really Perform Bad for Graph Representation?")]),r._v(" "),e("p",[r._v("Transformer在自然语言、计算机视觉上都取得了巨大的成功，但是在图数据上还没法达到图卷积的效果。直觉上来说，这是因为我们可以认为Transformer也是一种图卷积，不同节点（token）通过Self-attention得到的注意力矩阵可以被认为是图卷积中用来聚合节点特征的边权重，但是如果将原始的Transformer直接用于图，就会丢失了图的以下几个关键信息：")]),r._v(" "),e("ol",[e("li",[r._v("节点在在图中的重要性")]),r._v(" "),e("li",[r._v("空间拓扑结构信息")]),r._v(" "),e("li",[r._v("边信息")])]),r._v(" "),e("p",[r._v("所以 Graphormer 主要在原始的 Transformer 中融入更多的特征来补充上边几个信息。")]),r._v(" "),e("ClientOnly",[e("leave")],1)],1)}),[],!1,null,null,null);t.default=v.exports}}]);