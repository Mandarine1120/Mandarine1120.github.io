(window.webpackJsonp=window.webpackJsonp||[]).push([[25],{376:function(t,r,e){"use strict";e.r(r);var a=e(4),n=Object(a.a)({},(function(){var t=this,r=t.$createElement,e=t._self._c||r;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"文本图"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#文本图"}},[t._v("#")]),t._v(" 文本图")]),t._v(" "),e("p",[t._v("在自然语言处理领域，文本图（Text Graph，或Textual Graph）指的是将文本以图的形式进行表示，该方法通常用于文本理解、文本分类、文本摘要、关系抽取、文本蕴含任务等。")]),t._v(" "),e("p",[t._v("文本图的节点和边的定义很宽泛。对于节点来说，其可以被视为单个单词或句子，或特定于领域的术语，或文本中提到的实体（此时退化为关系图）。对于边来说，其可以是共现关系、上下文关系、从属关系等等。")]),t._v(" "),e("ClientOnly",[e("title-pv")],1),t._v(" "),e("h2",{attrs:{id:"text-gcn"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#text-gcn"}},[t._v("#")]),t._v(" Text GCN")]),t._v(" "),e("p",[t._v("论文题目：Graph Convolutional Networks for Text Classification")]),t._v(" "),e("p",[t._v("论文将图卷积神经网络GCN用于文本分类任务，构造网络模型text GCN，并在4个长文本数据集（20NG，R8,R52,Ohsumed）和1个短文本数据集(MR,电影短评论数据集)上进行了实验验证。")]),t._v(" "),e("p",[t._v("首先，论文构建了一个大型的异构文本图网络，图中包含单词节点和文档节点，以至于全局共现词可以被明确的构建出。如下图所示，文本图网络中的节点的数量为文档节点个数（数据集的大小）加上数据集中包含的不重复单词的个数（词汇表的大小）。")]),t._v(" "),e("p",[t._v("之后，论文在GCN基础上进行的改造，主要改造在于提出使用整个语料库构造成一个图，图中的节点是语料库中的每个document 和字典中所有的word，构建一个大图，并用邻接矩阵表示。构建好图及其邻接矩阵之后，使用GCN进行传递计算。")]),t._v(" "),e("p",[t._v("最后，文章使用两层GCN进行模型训练，并通过实验证明两层GCN比单层的或者两层以上GCN效果更好。最后作者在文章中对网络参数的选择进行了实验对比，也对模型的词向量学习进行了可视化表现，在多方面证明了GCN在文档分类上的有效性。这说明GCN中的图及其邻接矩阵可以用多种方式进行构建，比如词之间的相关程度，词与文档的相关程度，甚至是依赖树等任何你能想到的方式，所以提出新的图构建方式也是GCN在文档分类上的一种创新。")]),t._v(" "),e("h2",{attrs:{id:"text-transformer"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#text-transformer"}},[t._v("#")]),t._v(" Text Transformer")]),t._v(" "),e("p",[t._v("论文题目：GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph")]),t._v(" "),e("p",[t._v("Transformer在自然语言、计算机视觉上都取得了巨大的成功，但是在图数据上还没法达到图卷积的效果。直觉上来来说，这是因为我们可以认为Transformer也是一种图卷积，不同节点（token）通过Self-attention得到的注意力矩阵可以被认为是图卷积中用来聚合节点特征的边权重，但是如果将原始的Transformer直接用于图，就会丢失了图的以下几个关键信息：")]),t._v(" "),e("ol",[e("li",[t._v("节点在在图中的重要性")]),t._v(" "),e("li",[t._v("空间拓扑结构信息")]),t._v(" "),e("li",[t._v("边信息")])]),t._v(" "),e("p",[t._v("所以 Graphformer 主要在原始的 Transformer 中融入更多的特征来补充上边几个信息。")]),t._v(" "),e("ClientOnly",[e("leave")],1)],1)}),[],!1,null,null,null);r.default=n.exports}}]);