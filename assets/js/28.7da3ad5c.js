(window.webpackJsonp=window.webpackJsonp||[]).push([[28],{381:function(e,t,n){"use strict";n.r(t);var i=n(4),a=Object(i.a)({},(function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("h1",{attrs:{id:"text-graph"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#text-graph"}},[e._v("#")]),e._v(" Text Graph")]),e._v(" "),n("ClientOnly",[n("title-pv")],1),e._v(" "),n("p",[e._v("In the Natural language processing field, text graph or textual graph is a form of data model representing textual information and semantic relationship. It models the structure and semantics of text by representing words, phrases, or sentences as nodes in the graph, and using edges to represent their association relationships.")]),e._v(" "),n("p",[e._v("Text graphs can capture various relationships within a text, such as co-occurrence relationships between words, similarity between sentences, correlations between concepts, contextual relationships, synonymous relationships, superlative relationships, etc. By analyzing and modeling these relationships in text, text graphs can provide more comprehensive semantic information and richer contextual understanding. This method is commonly used for text NLU, text classification, text summarization, relationship extraction, text implication tasks, etc.")]),e._v(" "),n("p",[e._v("The definition of nodes and edges in text graphs is very broad. For nodes, they can be considered as individual words or sentences, domain specific terms, or entities mentioned in the text (which in this case devolves into relationship diagrams). For edges, they can be co-occurrence relationships, contextual relationships, subordinate relationships, and so on.")]),e._v(" "),n("p",[e._v("Using textual graphs offers several benefits:")]),e._v(" "),n("ul",[n("li",[e._v("Capturing Semantic Associations: Textual graphs can capture semantic relationships within text, including associations between words, relevance between phrases, and similarity between sentences. By modeling these semantic relationships, textual graphs provide more comprehensive semantic information, aiding models in better understanding the meaning of text.")]),e._v(" "),n("li",[e._v("Enhancing Contextual Understanding: Textual graphs can establish connections within the context of sentences or paragraphs, linking related information through edge associations. This helps models better comprehend information within the context, capturing relationships between contexts and improving the accurate understanding of the text.")]),e._v(" "),n("li",[e._v("Integrating Multimodal Information: Textual graphs can be combined with information from other modalities such as images and audio, forming multimodal graphs. This integration is particularly useful when dealing with multimodal data, enabling a better representation and modeling of relationships between text and other modalities, achieving a more comprehensive understanding of information.")]),e._v(" "),n("li",[e._v("Semantic Inference and Knowledge Extraction: Through textual graphs, semantic inference and knowledge extraction can be performed. For example, by analyzing relationships within the textual graph, one can engage in inference and deduction, uncovering hidden semantic relationships. Furthermore, textual graphs can extract essential concepts and key information from text, which can be utilized for knowledge representation and the construction of knowledge graphs.")])]),e._v(" "),n("h2",{attrs:{id:"textgcn"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#textgcn"}},[e._v("#")]),e._v(" TextGCN")]),e._v(" "),n("p",[e._v("论文题目："),n("RouterLink",{attrs:{to:"/views/Paper/1-TextGCN.html"}},[e._v("TextGCN：Graph Convolutional Networks for Text Classification")])],1),e._v(" "),n("p",[e._v("该文章基于文本的共现关系构建了文本图、使用one-hot编码作为初始特征。论文将图卷积神经网络GCN用于文本分类任务，构造网络模型Text GCN，并在4个长文本数据集（20NG，R8,R52,Ohsumed）和1个短文本数据集(MR,电影短评论数据集)上进行了实验验证。")]),e._v(" "),n("p",[e._v("首先，论文构建了一个大型的异构文本图网络，图中包含单词节点和文档节点，以至于全局共现词可以被明确的构建出。如下图所示，文本图网络中的节点的数量为文档节点个数（数据集的大小）加上数据集中包含的不重复单词的个数（词汇表的大小）。")]),e._v(" "),n("p",[e._v("之后，论文在GCN基础上进行的改造，主要改造在于提出使用整个语料库构造成一个图，图中的节点是语料库中的每个document 和字典中所有的word，构建一个大图，并用邻接矩阵表示。构建好图及其邻接矩阵之后，使用GCN进行传递计算。")]),e._v(" "),n("p",[e._v("最后，文章使用两层GCN进行模型训练，并通过实验证明两层GCN比单层的或者两层以上GCN效果更好。最后作者在文章中对网络参数的选择进行了实验对比，也对模型的词向量学习进行了可视化表现，在多方面证明了GCN在文档分类上的有效性。这说明GCN中的图及其邻接矩阵可以用多种方式进行构建，比如词之间的相关程度，词与文档的相关程度，甚至是依赖树等任何你能想到的方式，所以提出新的图构建方式也是GCN在文档分类上的一种创新。")]),e._v(" "),n("h2",{attrs:{id:"bertgcn"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#bertgcn"}},[e._v("#")]),e._v(" BertGCN")]),e._v(" "),n("p",[e._v("论文题目："),n("RouterLink",{attrs:{to:"/views/Paper/2-BertGCN.html"}},[e._v("BertGCN: Transductive Text Classification by Combining GCN and BERT")])],1),e._v(" "),n("p",[e._v("BertGCN也是两部分Build Graph及Model Train。")]),e._v(" "),n("p",[e._v("1.构图：Build Graph这部分与TextGCN是一样的，没啥区别就略过了，重点来聊聊模型部分。")]),e._v(" "),n("p",[e._v("2.初始化：BertGCN在node embedding初始化时，不是采用随机初始化策略，而是用bert处理document节点进行初始化，而word节点初始embedding直接初始化为0。")]),e._v(" "),n("p",[e._v("3.融合BERT模块与GCN模块得到的结果、模型训练")]),e._v(" "),n("p",[e._v("在融合Bert与GCN训练这部分，文中指出，将Bert encoder部分得到embedding后丢进GCN里，直接联合训练，会有两个问题出现，")]),e._v(" "),n("ol",[n("li",[e._v("梯度回传时，Bert部分得不到有效的梯度优化。")]),e._v(" "),n("li",[e._v("GCN是全图更新的，假设图是1w个document节点，则bert部分1w个document同时进行bert encoder得到document embedding,然后丢到GCN layer中更新训练，这显然是做不到的。")])]),e._v(" "),n("p",[e._v("针对这些问题，文中提出了插值更新的方法。最后的插值更新就是将GCN那部分得到document embedding的及Bert单独作用于文本的两个document embedding加起来得到融合分类概率Z，然后采用cross entropy损失函数做一个分类预测。")]),e._v(" "),n("p",[e._v("由于BERT的存在，BertGCN在训练时只能一次加载一个batch而非整个图。为此，BertGCN使用了记忆存储库（Memory Bank） 技术解决该问题。记忆存储库\n保存了所有文档结点的特征，将图结点与训练时的每个batch进行分离，每个batch只需要从中取所需的一小部分结点特征即可。")]),e._v(" "),n("p",[e._v("简要来说，记忆存储机制通过每次迭代，动态地更新一小部分文档结点，使用这部分结点训练模型。这就避免了一次性将所有特征读入BERT进行计算，极大减少了内存开销。")]),e._v(" "),n("p",[e._v("但由此带来的一个问题是，由于文档结点是分批更新的，所以在一个Epoch的不同迭代步，输入到模型里的特征会出现不一致的现象。为此，BertGCN在更新BERT模块时采用较小的学习率，减少特征之间的不一致。为加速训练，BertGCN还在训练之前用一个在下游数据集上训练好的BERT模型初始化BertGCN中的BERT模块。")]),e._v(" "),n("ClientOnly",[n("leave")],1)],1)}),[],!1,null,null,null);t.default=a.exports}}]);